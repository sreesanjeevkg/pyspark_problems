{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d06dd51-b835-4c5b-8ba5-90f501870b68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# You have a DataFrame containing a list of events for each user. Your task is to calculate the duration (in hours) of each complete sequence from start to complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f07006a-482c-427d-92fc-22373246d01c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, min, max, unix_timestamp, rand, expr, col, when, monotonically_increasing_id, countDistinct, count\n",
    "from pyspark.sql.types import IntegerType, StringType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35567ac8-408e-4528-aca1-21a6b5756f6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"101\").getOrCreate()\n",
    "\n",
    "num_rows = 1200000\n",
    "\n",
    "\n",
    "df = spark.range(0, num_rows)\n",
    "df = df.withColumn(\"event_id\", (rand() * 400000).cast(IntegerType()) + 1)\n",
    "df = df.withColumn(\"timestamp\", expr(\"to_timestamp('2023-01-01 00:00:00') + interval 1 year * rand()\"))\n",
    "\n",
    "# Create event types\n",
    "event_types = [\"event_start\", \"event_process\", \"event_complete\"]\n",
    "\n",
    "df = df.withColumn(\"event_type\", expr(f\"array({', '.join(repr(t) for t in event_types)})[int(rand() * {len(event_types)})]\"))\n",
    "\n",
    "# Ensure each event_id has all three event types\n",
    "df = df.withColumn(\"row_num\", expr(\"row_number() over (partition by event_id order by timestamp)\"))\n",
    "df = df.withColumn(\"event_type\", when(col(\"row_num\") == 1, \"event_start\")\n",
    "                                .when(col(\"row_num\") == 2, \"event_process\")\n",
    "                                .when(col(\"row_num\") == 3, \"event_complete\"))\n",
    "\n",
    "# Adjust timestamps to ensure correct order\n",
    "df = df.withColumn(\"timestamp\", \n",
    "                   when(col(\"event_type\") == \"event_start\", col(\"timestamp\"))\n",
    "                   .when(col(\"event_type\") == \"event_process\", expr(\"timestamp + interval 5 minutes\"))\n",
    "                   .when(col(\"event_type\") == \"event_complete\", expr(\"timestamp + interval 15 minutes\")))\n",
    "\n",
    "# Select and order final columns\n",
    "df = df.select(\"event_id\", \"event_type\", \"timestamp\").orderBy(\"event_id\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "163cb583-8720-462f-8c30-f0e10493a3a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------------+\n|event_id| event_type|          timestamp|\n+--------+-----------+-------------------+\n|      34|event_start|2023-10-01 00:00:00|\n+--------+-----------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.event_id == 34).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbf97646-34cd-4413-baac-18743510db5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filtered_id = df.\\\n",
    "    filter(df.event_type.isin(\"event_start\", \"event_complete\")).groupBy(\"event_id\").agg(\n",
    "    count(\"event_id\").alias(\"event_count\")\n",
    "        ).filter(col(\"event_count\") == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8db74cd-d79d-4721-8c89-93fa549fc61b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n|event_id|duration|\n+--------+--------+\n|      27| 1464.25|\n|      28| 2928.25|\n|      31| 5088.25|\n|      53| 4416.25|\n|      78| 7296.25|\n|      81| 4416.25|\n|     101| 2208.25|\n|     103| 7272.25|\n|     108| 5088.25|\n|     115| 1464.25|\n|     126| 1416.25|\n|     148| 5088.25|\n|     155| 5856.25|\n|     183| 1416.25|\n|     210|  744.25|\n|     211| 5808.25|\n|     236| 3672.25|\n|     243| 4416.25|\n|     251| 2136.25|\n|     253| 5856.25|\n+--------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_aliased = df.alias(\"main\")\n",
    "filtered_id_aliased = filtered_id.alias(\"filtered\")\n",
    "\n",
    "filtered_df = df_aliased.\\\n",
    "    filter(df_aliased.timestamp.isNotNull()).\\\n",
    "    filter((df_aliased.event_type == \"event_start\") | (df_aliased.event_type == \"event_complete\")).\\\n",
    "    join(\n",
    "    filtered_id_aliased,\n",
    "    df_aliased.event_id == filtered_id_aliased.event_id,\n",
    "    \"inner\"\n",
    ").select(df_aliased.event_id, df_aliased.event_type, df_aliased.timestamp)\n",
    "    \n",
    "\n",
    "result = filtered_df.groupBy(filtered_df.event_id).agg(\n",
    "    min(filtered_df.timestamp).alias(\"start_time\"),\n",
    "    max(filtered_df.timestamp).alias(\"end_time\")\n",
    ").withColumn(\"duration\", (unix_timestamp(\"end_time\") - unix_timestamp(\"start_time\")) / 3600).\\\n",
    "select(filtered_df.event_id, \"duration\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark problem - 101",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
